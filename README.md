# Nuppeppou

This is Nuppeppou, a Battlesnake implemented in Python. It's deployed with [Replit](https://repl.it).

## Technologies Used

* [Python3](https://www.python.org/)
* [Flask](https://flask.palletsprojects.com/)


## Profile

* [Daedalean](https://play.battlesnake.com/u/daedalean/)

---

## Customizations

Nuppeppou presently uses these settings for personalizing its appearance:

```python
return {
    "apiversion": "1",
    "author": "daedalean",
    "color": "#E80978",
    "head": "pixel",
    "tail": "pixel",
    "version": "0.0.1-beta",
}

```

## Behavior

On the start of each game, Nuppeppou initializes or recycles an [efficiently updatable neural network](https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network), except when 4 efficiently updatable neural networks were previously initialized and none are available for recycling. 

The input feature set for the efficiently updatable neural networks are (game_rules, board_size, x, y, player_type), (game_rules, board_size, x, y, player_type, piece_type), and (game_rules, board_size, x, y, player_type, piece_type, health) tuples. There are 6 game rules: Standard, Constrictor, Battlesnake Royale, Wrapped, Solo, and Squad. There are 3 board sizes: small (7×7), medium (11×11), and large (19×19). There are 3 player types: you, squadmates, and opponents. There are 8 piece types: a head, a body segment moving left, a body segment moving right, a body segment moving down, a body segment moving up, a body segment under another segment, food, and hazard. There are 100 health values, from 1 to 100. Therefore, there are 1009962 such tuples. If the game rules are G and the board size is B and there is a player type P and head Q with health H on the square with coordinates (x, y), then we set the input (G, B, x, y, P, Q, H) to 1. Otherwise, we set it to 0.

The layers used in the efficiently updatable neural networks are three linear layers, 1009962→256, 256→256, 256→4. All layers are linear and all hidden neurons use the ReLU activation function.

The efficiently updatable neural networks were trained with an offline off-policy Monte Carlo method. In particular, a training data set, validation data set, and test dat set were generated by creating games with Battlesnakes that remove the move that moves themselves back on their own neck from possibility and otherwise selects moves randomly. The inputs were the input feature sets for states in the created games. The targets were in WDL-space. By WDL-space, we mean loss=0.0, draw=0.5, and win=1.0. The target for a (state, action) tuple was the greatest target for that tuple in the training dataset. The efficiently updatable neural networks output a prediction for each move. Gradients were calculated only for the actions played in the states in the created games.

On every turn of each game, Nuppeppou first removes moves that move Nuppeppou back on its own neck, hit walls, hit itself, and collide with others from possibility. If all moves are removed from possibility, then Nuppeppou moves up. Otherwise, if an efficiently updatable neural network is assigned to Nuppeppou for this game, then Nuppeppou uses the the efficiently updatable neural network to get scores for each move and selects the possible move assigned the greatest score. Otherwise, Nuppeppou moves randomly.

On the end of each game, Nuppeppou deallocates some server-side resources. In particular, Nuppeppou deallocates memory of the previous state that was needed for updating accumulators in efficiently updatable neural networks. Nuppeppou does not deallocate efficiently updatable neural networks, but moves them to a stack of  efficiently updatable neural networks to be recycled.
